{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../malayalam_chars_mapping.pkl\", \"rb\") as f:\n",
    "    MALAYALAM_CHARS_MAPPING = pickle.load(f)\n",
    "\n",
    "with open(\"../malayalam_syllabeles_mapping.pkl\", \"rb\") as f:\n",
    "    MALAYALAM_SYLLABELES_MAPPING = pickle.load(f)\n",
    "\n",
    "with open(\"../merged_chars_mapping.pkl\", \"rb\") as f:\n",
    "    MERGED_CHARS_MAPPING = pickle.load(f)\n",
    "\n",
    "with open(\"../vocabulary.pkl\", \"rb\") as f:\n",
    "    VOCABULARY = pickle.load(f)\n",
    "\n",
    "with open(\"../dataset/output.json\", \"r\") as f:\n",
    "    ds = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "Maps a given string to a list of ints.\n",
    "\n",
    "- The python encode(\"utf-8\") method maps a given text to list of integer ids.\n",
    "- merge_malayalam_syllabele_tokens() method joins the ids to new id for ka, kaa, ki ,kee...\n",
    "- merge_malayalam_char_tokens() method joins the remaining malayalam chars.\n",
    "- merge_pair() method joins the ids according to subword mapping.\n",
    "- encode() method uses all the above methods and convert text to list of ids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_malayalam_char_tokens(tokens: list[int]) -> list[int]:\n",
    "    \"\"\"Merge UTF-8 byte sequences into new vocab ids for Malayalam characters\"\"\"\n",
    "    merged_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i + 2 < len(tokens):  # check if 3 bytes available\n",
    "            key = (tokens[i], tokens[i+1], tokens[i+2])\n",
    "            value = MALAYALAM_CHARS_MAPPING.get(key)\n",
    "            if value is not None:\n",
    "                merged_tokens.append(value)\n",
    "                i += 3\n",
    "                continue\n",
    "        # fallback: keep single byte\n",
    "        merged_tokens.append(tokens[i])\n",
    "        i += 1\n",
    "    return merged_tokens\n",
    "\n",
    "def merge_malayalam_syllabele_tokens(tokens: list[int]) -> list[int]:\n",
    "    \"\"\"Merge UTF-8 byte sequences into new vocab ids for Malayalam characters\"\"\"\n",
    "    merged_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i + 5 < len(tokens):  # check if 3 bytes available\n",
    "            key = (tokens[i], tokens[i+1], tokens[i+2], tokens[i+3], tokens[i+4], tokens[i+5])\n",
    "            value = MALAYALAM_SYLLABELES_MAPPING.get(key)\n",
    "            if value is not None:\n",
    "                merged_tokens.append(value)\n",
    "                i += 6\n",
    "                continue\n",
    "        # fallback: keep single byte\n",
    "        merged_tokens.append(tokens[i])\n",
    "        i += 1\n",
    "    return merged_tokens\n",
    "\n",
    "def merge_pair(tokens, pair_to_merge, new_idx):\n",
    "    \"\"\"Merge common pairs to create new pairs\"\"\"\n",
    "    merged_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens)-1 and (tokens[i], tokens[i+1]) == pair_to_merge:\n",
    "            merged_tokens.append(new_idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            merged_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    return merged_tokens\n",
    "\n",
    "def encode(text: str) -> list[int]:\n",
    "    \"\"\"Converts text to a list of token ids\"\"\"\n",
    "    tokens = list(text.encode(\"utf-8\")) # text of list of ids 0-225\n",
    "    tokens = merge_malayalam_syllabele_tokens(tokens) # Merge malayalam syllabele tokens\n",
    "    tokens = merge_malayalam_char_tokens(tokens) # Merge malayalam char tokens\n",
    "\n",
    "    for pair_to_merge, new_idx in MERGED_CHARS_MAPPING.items():\n",
    "        tokens = merge_pair(tokens, pair_to_merge, new_idx)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids: list[int]) -> str:\n",
    "    text = \"\"\n",
    "    for id in ids:\n",
    "        if id < 256:\n",
    "            text += chr(id)\n",
    "        else:\n",
    "            text += VOCABULARY.get(id)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16212])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\".join(ds[:1000])\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14590, 1622)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data to train and val\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3012, 1001, 9302,  890, 6839, 7366,  708, 7367, 2677])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8 # Context length\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([3012]), the target is 1001\n",
      "When input is tensor([3012, 1001]), the target is 9302\n",
      "When input is tensor([3012, 1001, 9302]), the target is 890\n",
      "When input is tensor([3012, 1001, 9302,  890]), the target is 6839\n",
      "When input is tensor([3012, 1001, 9302,  890, 6839]), the target is 7366\n",
      "When input is tensor([3012, 1001, 9302,  890, 6839, 7366]), the target is 708\n",
      "When input is tensor([3012, 1001, 9302,  890, 6839, 7366,  708]), the target is 7367\n",
      "When input is tensor([3012, 1001, 9302,  890, 6839, 7366,  708, 7367]), the target is 2677\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context}, the target is {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8]), torch.Size([4, 8]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4 # how many independent sequence we process in parallel?\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is [9928], the target is 884\n",
      "When input is [9928, 884], the target is 3184\n",
      "When input is [9928, 884, 3184], the target is 974\n",
      "When input is [9928, 884, 3184, 974], the target is 3188\n",
      "When input is [9928, 884, 3184, 974, 3188], the target is 1230\n",
      "When input is [9928, 884, 3184, 974, 3188, 1230], the target is 9391\n",
      "When input is [9928, 884, 3184, 974, 3188, 1230, 9391], the target is 891\n",
      "When input is [9928, 884, 3184, 974, 3188, 1230, 9391, 891], the target is 1842\n",
      "When input is [7785], the target is 1491\n",
      "When input is [7785, 1491], the target is 1635\n",
      "When input is [7785, 1491, 1635], the target is 261\n",
      "When input is [7785, 1491, 1635, 261], the target is 853\n",
      "When input is [7785, 1491, 1635, 261, 853], the target is 2886\n",
      "When input is [7785, 1491, 1635, 261, 853, 2886], the target is 2569\n",
      "When input is [7785, 1491, 1635, 261, 853, 2886, 2569], the target is 4516\n",
      "When input is [7785, 1491, 1635, 261, 853, 2886, 2569, 4516], the target is 1260\n",
      "When input is [5867], the target is 696\n",
      "When input is [5867, 696], the target is 1714\n",
      "When input is [5867, 696, 1714], the target is 567\n",
      "When input is [5867, 696, 1714, 567], the target is 4486\n",
      "When input is [5867, 696, 1714, 567, 4486], the target is 6916\n",
      "When input is [5867, 696, 1714, 567, 4486, 6916], the target is 2124\n",
      "When input is [5867, 696, 1714, 567, 4486, 6916, 2124], the target is 3075\n",
      "When input is [5867, 696, 1714, 567, 4486, 6916, 2124, 3075], the target is 286\n",
      "When input is [505], the target is 888\n",
      "When input is [505, 888], the target is 2765\n",
      "When input is [505, 888, 2765], the target is 2787\n",
      "When input is [505, 888, 2765, 2787], the target is 1800\n",
      "When input is [505, 888, 2765, 2787, 1800], the target is 2823\n",
      "When input is [505, 888, 2765, 2787, 1800, 2823], the target is 1093\n",
      "When input is [505, 888, 2765, 2787, 1800, 2823, 1093], the target is 1429\n",
      "When input is [505, 888, 2765, 2787, 1800, 2823, 1093, 1429], the target is 1238\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When input is {context.tolist()}, the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10000])\n",
      "torch.Size([])\n",
      "tensor(9.8662, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets):\n",
    "\n",
    "        logits = self.token_embedding_table(idx) # This would be a B, T, C Batch, Time, Channel\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :] # becomes B,C\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # B, T+1\n",
    "        return idx\n",
    "\n",
    "vocab_size = sorted(VOCABULARY.keys())[-1]\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape), print(loss.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- ln"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "make-more",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
